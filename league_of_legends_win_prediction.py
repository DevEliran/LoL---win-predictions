# -*- coding: utf-8 -*-
"""League Of Legends  - Win Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k7LYwi5u2oNtsxx5XQto17Y3FLE1M7qm
"""

!pip install -q sklearn

pip install -q tensorflow>=2 tfds-nightly matplotlib

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x  # this line is not required unless you are in a notebook

# Colab library to upload files to notebook
from google.colab import files

# Install Kaggle library
!pip install -q kaggle

# Upload kaggle API key file
uploaded = files.upload()

!mkdir ~/.kaggle
!cp /content/kaggle.json ~/.kaggle/kaggle.json

!chmod 600 /root/.kaggle/kaggle.json

!!kaggle datasets download -d gyejr95/league-of-legends-challenger-ranked-games2020

!ls

# Commented out IPython magic to ensure Python compatibility.
from __future__ import absolute_import, division, print_function, unicode_literals
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import clear_output
from six.moves import urllib
import tensorflow.compat.v2.feature_column as fc
import tensorflow as tf
import pandas as pd
from zipfile import ZipFile
import seaborn as sns
# %matplotlib inline
sns.set_style('darkgrid')

zip_file = ZipFile('league-of-legends-challenger-ranked-games2020.zip')

dfs = {text_file.filename: pd.read_csv(zip_file.open(text_file.filename))
       for text_file in zip_file.infolist()
       if text_file.filename.endswith('.csv')}
      
# Import the test and train datasets into pandas dataframe
df_train_challenger = dfs['Challenger_Ranked_Games.csv']
df_test_challenger = dfs['Challenger_Ranked_Games.csv']

challenger_train = df_train_challenger[["blueWins"]]
challenger_test = df_test_challenger[["blueWins"]]

# challenger_train_blue = df_train_challenger.pop("blueWins")
# challenger_train_red = df_train_challenger.pop("redWins")


# challenger_test_blue = df_test_challenger.pop("blueWins")
# challenger_test_red = df_test_challenger.pop("redWins")

df_test_challenger.pop("gameId")

df_train_challenger.head()

challenger_train.head()

df_train_challenger.describe()

df_train_challenger.gameDuraton.hist(bins=50)

df_train_challenger.blueFirstBlood.value_counts().plot(kind='barh')

df_train_challenger.blueFirstDragon.value_counts().plot(kind='barh')

df_train_challenger.blueWardPlaced.hist(bins=50)

pd.concat([df_train_challenger, challenger_train], axis=1).groupby('blueFirstTower').blueWins.mean().plot(kind='barh').set_xlabel('% Blue Won')

pd.concat([df_train_challenger, challenger_train], axis=1).groupby('blueFirstBaron').blueWins.mean().plot(kind='barh').set_xlabel('% Blue Won')

blue_corr = df_train_challenger.corr()['blueWins'][:].sort_values(axis=0, ascending=False) 
red_corr = df_train_challenger.corr()['redWins'][:].sort_values(axis=0, ascending=False) 
# pearson method normalizes values for me
print(blue_corr)

corr_cols = [prop for prop,corr in blue_corr.iteritems() if abs(corr)>0.3]
plt.figure(figsize=(26,26))
sns.set(font_scale = 1)
sns.heatmap(df_train_challenger[corr_cols].corr(), annot=True, linewidths=.5, linecolor='black', cmap="BuPu")

corr_cols_2 = [prop for prop,corr in blue_corr.iteritems() if abs(corr)>0.5]
plt.figure(figsize=(12,12))
sns.set(font_scale = 1)
sns.heatmap(df_train_challenger[corr_cols_2].corr(), annot=True, linewidths=.5, linecolor='black', cmap="BuPu")

def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):
  def input_function():  # inner function, this will be returned
    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label
    if shuffle:
      ds = ds.shuffle(1000)  # randomize order of data
    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs
    return ds  # return a batch of the dataset
  return input_function  # return a function object for use

train_input_fn = make_input_fn(df_train_challenger, challenger_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model
eval_input_fn = make_input_fn(df_test_challenger, challenger_test, num_epochs=1, shuffle=False)

feature_columns = []
for feature_name in corr_cols_2:
  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))

print(feature_columns)

linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)

linear_est.train(train_input_fn)  # train
result = linear_est.evaluate(eval_input_fn)  # get model metrics/stats by testing on tetsing data

clear_output()
print(result['accuracy'])  # the result variable is simply a dict of stats about our model

# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.
classifier = tf.estimator.DNNClassifier(
    feature_columns=feature_columns,
    # Two hidden layers of 30 and 10 nodes respectively.
    hidden_units=[30, 10],
    # The model must choose between 3 classes.
    n_classes=2)

def input_fn(features, batch_size=256):
    # Convert the inputs to a Dataset without labels.
    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)
# features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']
features = ['blueTowerKills', 'blueFirstInhibitor', 'blueInhibitorKills', ]
predict = {}

print("Please type numeric values as prompted.")
for feature in features:
    val = input(feature + ": ")


predict[feature] = [float(val)]

predictions = classifier.predict(input_fn=lambda: input_fn(predict))
for pred_dict in predictions:
    class_id = pred_dict['class_ids'][0]
    probability = pred_dict['probabilities'][class_id]

    # print('Prediction is "{}" ({:.1f}%)'.format(
    #     [class_id], 100 * probability))

